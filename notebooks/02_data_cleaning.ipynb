{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TMDB Data Cleaning & Exploration\n",
    "\n",
    "This notebook handles the cleaning and preprocessing of raw TMDB movie data.\n",
    "\n",
    "## Objectives\n",
    "1. Load raw JSON data\n",
    "2. Flatten nested JSON columns\n",
    "3. Clean data types and handle missing values\n",
    "4. Feature engineering\n",
    "5. Export cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import ast\n",
    "\n",
    "# Add project root to path to access src\n",
    "sys.path.append('..')\n",
    "from src.utils.helpers import setup_logging\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Setup Logging\n",
    "logger = setup_logging(config_path='../config/config.yaml', module_name='notebook_cleaning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 1. Load Configuration & Data\n",
    "\n",
    "Load file paths from config and read raw JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting data cleaning process...\")\n",
    "\n",
    "# Load config\n",
    "try:\n",
    "    with open('../config/config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    raw_path = Path('..') / config['paths']['raw_data']\n",
    "    logger.info(f\"Raw data path: {raw_path}\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback if running from notebook dir without parent context\n",
    "    raw_path = Path('../data/raw')\n",
    "    logger.warning(f\"Config not found, using default path: {raw_path}\")\n",
    "\n",
    "# Load JSON files\n",
    "data_list = []\n",
    "if raw_path.exists():\n",
    "    json_files = list(raw_path.glob('*.json'))\n",
    "    logger.info(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                data_list.append(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file}: {e}\")\n",
    "else:\n",
    "    logger.error(\"Raw data directory does not exist!\")\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "logger.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_step1",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "### 2.1 Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drop_cols",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['adult', 'imdb_id', 'original_title', 'video', 'homepage']\n",
    "existing_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "df_clean = df.drop(columns=existing_cols).copy()\n",
    "logger.info(f\"Dropped columns: {existing_cols}\")\n",
    "logger.info(f\"New shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_json",
   "metadata": {},
   "source": [
    "### 2.2 Flatten Nested Columns\n",
    "\n",
    "Extract data from: `belongs_to_collection`, `genres`, `production_countries`, `production_companies`, `spoken_languages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(data):\n",
    "    \"\"\"Extract single name from dict.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return data.get('name')\n",
    "    return np.nan\n",
    "\n",
    "def extract_names_list(data, key='name', separator='|'):\n",
    "    \"\"\"Extract list of names from list of dicts.\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        names = [item.get(key) for item in data if isinstance(item, dict) and item.get(key)]\n",
    "        return separator.join(names) if names else np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Apply extractions\n",
    "logger.info(\"Flattening nested JSON columns...\")\n",
    "df_clean['collection_name'] = df_clean['belongs_to_collection'].apply(extract_name)\n",
    "df_clean['genres'] = df_clean['genres'].apply(lambda x: extract_names_list(x))\n",
    "df_clean['production_countries'] = df_clean['production_countries'].apply(lambda x: extract_names_list(x))\n",
    "df_clean['production_companies'] = df_clean['production_companies'].apply(lambda x: extract_names_list(x))\n",
    "df_clean['spoken_languages'] = df_clean['spoken_languages'].apply(lambda x: extract_names_list(x))\n",
    "\n",
    "# Inspect results\n",
    "df_clean[['genres', 'collection_name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datatypes",
   "metadata": {},
   "source": [
    "### 2.3 Handle Missing & Incorrect Data\n",
    "\n",
    "1. Convert datatypes\n",
    "2. Replace unrealistic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Cleaning datatypes...\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['budget', 'id', 'popularity', 'revenue', 'vote_count', 'vote_average', 'runtime']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Convert release_date\n",
    "if 'release_date' in df_clean.columns:\n",
    "    df_clean['release_date'] = pd.to_datetime(df_clean['release_date'], errors='coerce')\n",
    "\n",
    "# Handle zero values in budget/revenue/runtime\n",
    "for col in ['budget', 'revenue', 'runtime']:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].replace(0, np.nan)\n",
    "\n",
    "# Create million USD columns\n",
    "df_clean['budget_musd'] = df_clean['budget'] / 1_000_000\n",
    "df_clean['revenue_musd'] = df_clean['revenue'] / 1_000_000\n",
    "\n",
    "# Handle text placeholders\n",
    "text_cols = ['overview', 'tagline']\n",
    "placeholders = ['No Data', 'No Overview', 'n/a', 'nan']\n",
    "for col in text_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].replace(placeholders, np.nan)\n",
    "\n",
    "logger.info(\"Datatypes cleaned.\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filtering",
   "metadata": {},
   "source": [
    "### 2.4 Filtering\n",
    "\n",
    "1. Remove duplicates\n",
    "2. Drop rows with unknown id/title\n",
    "3. Threshold filtering (at least 10 non-NaN)\n",
    "4. Status filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter_rows",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Filtering data...\")\n",
    "\n",
    "# Drop duplicates\n",
    "initial_len = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# Drop missing ID/Title\n",
    "df_clean = df_clean.dropna(subset=['id', 'title'])\n",
    "\n",
    "# Threshold filtering (keep rows with >= 10 non-nulls)\n",
    "df_clean = df_clean.dropna(thresh=10)\n",
    "\n",
    "# Status filtering\n",
    "if 'status' in df_clean.columns:\n",
    "    df_clean = df_clean[df_clean['status'] == 'Released']\n",
    "    df_clean = df_clean.drop(columns=['status'])\n",
    "\n",
    "rows_removed = initial_len - len(df_clean)\n",
    "logger.info(f\"Rows removed: {rows_removed}\")\n",
    "logger.info(f\"Final count: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Performing feature engineering...\")\n",
    "\n",
    "# Extract Cast & Crew info (Basic extraction)\n",
    "def extract_cast_info(credits_data):\n",
    "    if isinstance(credits_data, dict):\n",
    "        cast = credits_data.get('cast', [])\n",
    "        crew = credits_data.get('crew', [])\n",
    "        \n",
    "        # Top 5 cast\n",
    "        top_cast = [p.get('name') for p in cast[:5]]\n",
    "        cast_str = '|'.join(top_cast) if top_cast else np.nan\n",
    "        \n",
    "        # Director\n",
    "        director = next((p.get('name') for p in crew if p.get('job') == 'Director'), np.nan)\n",
    "        \n",
    "        return pd.Series([cast_str, len(cast), director, len(crew)])\n",
    "    return pd.Series([np.nan, 0, np.nan, 0])\n",
    "\n",
    "if 'credits' in df_clean.columns:\n",
    "    df_clean[['cast', 'cast_size', 'director', 'crew_size']] = df_clean['credits'].apply(extract_cast_info)\n",
    "\n",
    "# Release Year\n",
    "df_clean['release_year'] = df_clean['release_date'].dt.year\n",
    "\n",
    "# Force specific columns to string type\n",
    "string_cols = ['tagline', 'title', 'collection_name']\n",
    "for col in string_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype(str).replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspection",
   "metadata": {},
   "source": [
    "### 3.1 Data Anomaly Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "value_counts_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect extracted columns\n",
    "logger.info(\"Inspecting column values for anomalies...\")\n",
    "inspection_cols = ['genres', 'collection_name', 'production_countries', 'production_companies', 'spoken_languages']\n",
    "for col in inspection_cols:\n",
    "    if col in df_clean.columns:\n",
    "        logger.info(f\"\\n--- Top 10 values for {col} ---\")\n",
    "        print(df_clean[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finalizing",
   "metadata": {},
   "source": [
    "## 4. Finalize & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reorder_save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "desired_order = [\n",
    "    'id', 'title', 'tagline', 'release_date', 'genres', 'collection_name', \n",
    "    'original_language', 'budget_musd', 'revenue_musd', 'production_companies', \n",
    "    'production_countries', 'vote_count', 'vote_average', 'popularity', \n",
    "    'runtime', 'overview', 'spoken_languages', 'poster_path', \n",
    "    'cast', 'cast_size', 'director', 'crew_size'\n",
    "]\n",
    "\n",
    "# Select existing columns\n",
    "final_cols = [c for c in desired_order if c in df_clean.columns]\n",
    "df_final = df_clean[final_cols].copy()\n",
    "\n",
    "# Reset index\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Final columns: {df_final.columns.tolist()}\")\n",
    "\n",
    "# Save\n",
    "processed_path = Path('../data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = processed_path / 'movies_cleaned.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "logger.info(f\"Saved to {output_file}\")\n",
    "\n",
    "# Also save as parquet for efficient loading\n",
    "df_final.to_parquet(processed_path / 'movies_cleaned.parquet', index=False)\n",
    "logger.info(f\"Saved to parquet.\")\n",
    "logger.info(\"Data cleaning completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
